{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Author: ML Tlachac\n",
    "#Paper: Depression Screening from Text Message Reply Latency\n",
    "#year: 2020\n",
    "#github.com/mltlachac/EMBC2020\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moodable: (308498, 4)\n",
      "PHQ: (510, 3)\n",
      "unique PHQ: (501, 2)\n",
      "Moodable with PHQ: (261278, 7)\n",
      "EMU: (13438, 4)\n",
      "EMU last session: (12961, 6)\n",
      "PHQ: (126, 3)\n",
      "unique PHQ: (115, 2)\n",
      "EMU with PHQ: (12961, 7)\n"
     ]
    }
   ],
   "source": [
    "modality = \"Texts\"\n",
    "ndays = 14 #14 #28, #42, #364\n",
    "\n",
    "#load Moodable data\n",
    "dft1 = pd.read_csv('dataTextMoodableCommaQuoteWindows.csv', encoding = \"utf-8\")\n",
    "dft1 = dft1.dropna().reset_index()\n",
    "print(\"Moodable: \" + str(dft1.shape))\n",
    "\n",
    "#attach date data collected\n",
    "dfids1 = pd.read_csv(\"idsMoodableCommaQuoteWindows.csv\")\n",
    "dft1 = pd.merge(dfids1, dft1, on = \"id\")\n",
    "dft1 = dft1.reset_index()\n",
    "\n",
    "#attach PHQ-9 scores\n",
    "dft19 = pd.read_csv('phqsMoodableCommaQuoteWindows.csv')\n",
    "print(\"PHQ: \" + str(dft19.shape))\n",
    "\n",
    "scores = []\n",
    "for i in range(0, dft19.shape[0]):\n",
    "    content = dft19.content[i][1:-1].split(\",\")\n",
    "    summation = 0\n",
    "    for j in range(0, 9):\n",
    "        summation = summation + int(content[j][-2])\n",
    "    scores.append(summation)\n",
    "dft19[\"scores\"] = scores\n",
    "\n",
    "#limit to unique users, highest score is used\n",
    "idlist = []\n",
    "newScores = []\n",
    "for i in set(dft19[\"id\"]):\n",
    "    tempdf = dft19[dft19[\"id\"] == i].reset_index()\n",
    "    idlist.append(tempdf[\"id\"][0])\n",
    "    if tempdf.shape[0] > 1:\n",
    "        newScores.append(max(tempdf[\"scores\"]))\n",
    "    else:\n",
    "        newScores.append(tempdf.scores[0])\n",
    "unique19 = pd.DataFrame()\n",
    "unique19[\"id\"] = idlist\n",
    "unique19[\"scores\"] = newScores\n",
    "print(\"unique PHQ: \" + str(unique19.shape))\n",
    "\n",
    "dft1 = pd.merge(dft1, unique19, on = \"id\")\n",
    "print(\"Moodable with PHQ: \" + str(dft1.shape))\n",
    "\n",
    "#make ids represent dataset\n",
    "newids = []\n",
    "for m in range(0, dft1.shape[0]):\n",
    "    newids.append(\"m\" + str(dft1[\"id\"][m]))\n",
    "dft1[\"id\"] = newids\n",
    "\n",
    "#Load EMU data\n",
    "dft2 = pd.read_csv('dataTextEMUCommaQuoteWindows.csv', encoding = \"utf-8\")\n",
    "dft2 = dft2.dropna().reset_index()\n",
    "print(\"EMU: \" + str(dft2.shape))\n",
    "\n",
    "#attach date data collected, limit EMU participants to last session with each phone\n",
    "dfids2start = pd.read_csv('idsEMUCommaQuoteWindows.csv', encoding = \"utf-8\")\n",
    "dfids2 = pd.DataFrame()\n",
    "dfids2[\"id\"] = dfids2start.sessionid\n",
    "dfids2[\"date\"] = dfids2start.date\n",
    "dfids2[\"paid\"] = dfids2start.paid\n",
    "dft2 = pd.merge(dfids2, dft2, on = \"id\")\n",
    "dft2 = dft2[dft2.paid == 2]\n",
    "df2t = dft2.reset_index()\n",
    "print(\"EMU last session: \" + str(dft2.shape))\n",
    "\n",
    "#attach PHQ-9 scores\n",
    "dft29 = pd.read_csv('phqsEMUCommaQuoteWindows.csv')\n",
    "print(\"PHQ: \" + str(dft29.shape))\n",
    "\n",
    "scores = []\n",
    "for i in range(0, dft29.shape[0]):\n",
    "    content = dft29.content[i][1:-1].split(\",\")\n",
    "    summation = 0\n",
    "    for j in range(0, 9):\n",
    "        summation = summation + int(content[j][-2])\n",
    "    scores.append(summation)\n",
    "dft29[\"scores\"] = scores\n",
    "\n",
    "#limit to unique users, highest score is used\n",
    "idlist = []\n",
    "newScores = []\n",
    "for i in set(dft29[\"id\"]):\n",
    "    tempdf = dft29[dft29[\"id\"] == i].reset_index()\n",
    "    idlist.append(tempdf[\"id\"][0])\n",
    "    if tempdf.shape[0] > 1:\n",
    "        newScores.append(max(tempdf[\"scores\"]))\n",
    "    else:\n",
    "        newScores.append(tempdf.scores[0])\n",
    "        \n",
    "unique29 = pd.DataFrame()\n",
    "unique29[\"id\"] = idlist\n",
    "unique29[\"scores\"] = newScores\n",
    "print(\"unique PHQ: \" + str(unique29.shape))\n",
    "\n",
    "dft2 = pd.merge(dft2, unique29, on = \"id\")\n",
    "print(\"EMU with PHQ: \" + str(dft2.shape))\n",
    "\n",
    "#make ids represent dataset\n",
    "newids = []\n",
    "for e in range(0, dft2.shape[0]):\n",
    "    newids.append(\"e\" + str(dft2[\"id\"][e]))\n",
    "dft2[\"id\"] = newids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined: (274239, 5)\n",
      "(267735, 6)\n"
     ]
    }
   ],
   "source": [
    "dft1 = dft1.drop([\"level_0\"], axis = 1)\n",
    "dft2 = dft2.drop([\"paid\"], axis = 1)\n",
    "\n",
    "#Combine datasets\n",
    "dft = dft1.append(dft2)\n",
    "dft = dft.drop([\"index\"], axis = 1)\n",
    "print(\"Combined: \" + str(dft.shape))\n",
    "#remove duplicated data instances\n",
    "dft = dft.drop_duplicates()\n",
    "dft = dft.reset_index()\n",
    "print(dft.shape)\n",
    "\n",
    "#extract information from data instance metadata\n",
    "jsonExtract = []\n",
    "for i in range(0, len(dft.content)):\n",
    "    jsonExtract.append(json.loads(str(dft.content[i])))\n",
    "names = list(jsonExtract[0])\n",
    "jsonDF = pd.DataFrame()\n",
    "for n in names:\n",
    "    nlist = []\n",
    "    for i in range(0, len(dft.content)):\n",
    "        if n in list(jsonExtract[i]):\n",
    "            nlist.append(jsonExtract[i][n])\n",
    "        else:\n",
    "            nlist.append(\"-100\")\n",
    "    jsonDF[n+\"2\"] = nlist\n",
    "dft = pd.concat([dft, jsonDF], axis = 1)#, sort = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'id', 'date', 'type', 'content', 'scores', 'person2',\n",
      "       'safe_message2', 'from_address2', 'group_type2', 'seen2',\n",
      "       'service_center2', 'announcements_scenario_id2', 'svc_cmd2', 'date2',\n",
      "       'thread_id2', 'subject2', 'd_rpt_cnt2', 'protocol2', 'address2',\n",
      "       'hidden2', 'type2', 'error_code2', 'reserved2', 'read2',\n",
      "       'reply_path_present2', 'locked2', 'sim_slot2', 'date_sent2',\n",
      "       'announcements_subtype2', '_id2', 'pri2', 'sim_imsi2', 'favorite2',\n",
      "       'device_name2', 'app_id2', 'secret_mode2', 'link_url2', 'spam_report2',\n",
      "       'svc_cmd_content2', 'status2', 'msg_id2', 'delivery_date2', 'sub_id2',\n",
      "       'body2', 'callback_number2', 'teleservice_id2', 'group_id2',\n",
      "       'deletable2', 'using_mode2', 'creator2', 'roam_pending2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(dft.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(267732, 52)\n",
      "(267732, 53)\n",
      "(44601, 53)\n",
      "Number of Messages: 44601\n",
      "Number of Participants: 319\n"
     ]
    }
   ],
   "source": [
    "#participant-address combinations\n",
    "\n",
    "#remove data instances with no date\n",
    "dft = dft[dft.date2 != \"-1\"]\n",
    "print(dft.shape)\n",
    "dft = dft.reset_index()\n",
    "\n",
    "#Limit data to ndays\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser\n",
    "\n",
    "indexes = []\n",
    "for i in range(0, dft.shape[0]):\n",
    "    timeEnd = datetime.fromtimestamp(dft.date[i]/1000)\n",
    "    timeStart = timeEnd - timedelta(days=ndays)\n",
    "    timeCurrent = datetime.fromtimestamp(int(dft[\"date2\"][i])/1000)\n",
    "    diff = (timeStart-timeCurrent).days\n",
    "    if diff>0: #will be dropped\n",
    "        indexes.append(i)\n",
    "\n",
    "print(dft.shape)\n",
    "dft = dft.drop(indexes)\n",
    "dft = dft.drop([\"level_0\"], axis = 1)\n",
    "dft = dft.reset_index()\n",
    "print(dft.shape)\n",
    "\n",
    "print(\"Number of Messages: \" + str(dft.shape[0]))\n",
    "print(\"Number of Participants: \" + str(len(set(dft[\"id\"]))))\n",
    "\n",
    "#sort df by date\n",
    "dft = dft.sort_values(by = 'date2')\n",
    "\n",
    "#extract relevant data\n",
    "idlist = []\n",
    "addresslist = []\n",
    "datelist = []\n",
    "typelist = []\n",
    "bodylist = []\n",
    "scorelist = []\n",
    "for p in set(dft[\"id\"]):\n",
    "    tempdf = dft[dft[\"id\"]==p]\n",
    "    addresses = list(set(tempdf[\"address2\"]))\n",
    "    for i in range(0, len(addresses)):\n",
    "        idlist.append(p)\n",
    "        addresslist.append(i)\n",
    "        temptempdf = tempdf[tempdf[\"address2\"]==addresses[i]]\n",
    "        datelist.append(temptempdf[\"date2\"].tolist())\n",
    "        typelist.append(temptempdf[\"type2\"].tolist())\n",
    "        bodylist.append(temptempdf[\"body2\"].tolist())\n",
    "        scorelist.append(tempdf.scores.tolist()[0])\n",
    "\n",
    "newdf = pd.DataFrame()\n",
    "newdf[\"id\"] = idlist\n",
    "newdf[\"address\"] = addresslist\n",
    "newdf[\"date\"] = datelist\n",
    "newdf[\"type\"] = typelist\n",
    "newdf[\"body\"] = bodylist\n",
    "newdf[\"score\"] = scorelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All conversations: 5606\n",
      "Conversations with responses: 337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tlachacm\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:11: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    }
   ],
   "source": [
    "#limit to conversations with responses\n",
    "print(\"All conversations: \" + str(newdf.shape[0]))\n",
    "ids = []\n",
    "for i in range(0, newdf.shape[0]):\n",
    "    if \"1\" in newdf[\"type\"][i]:\n",
    "        start = newdf[\"type\"][i].index(\"1\") + 1\n",
    "        afterlist = newdf[\"type\"][i][start:]\n",
    "        if \"2\" in afterlist:\n",
    "            ids.append(i)\n",
    "\n",
    "newdf = newdf.ix[ids]\n",
    "newdf = newdf.reset_index()\n",
    "print(\"Conversations with responses: \" + str(newdf.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of responses: 3861\n"
     ]
    }
   ],
   "source": [
    "#extract latency of responses\n",
    "\n",
    "df = newdf\n",
    "latency = []\n",
    "count = 0\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    latlist = []\n",
    "    for j in range(1, len(df['date'][i])):\n",
    "        if df['type'][i][j] == '2':\n",
    "            if df['type'][i][j-1] == \"1\":\n",
    "                first = datetime.fromtimestamp(int(df['date'][i][j-1])/1000)\n",
    "                second = datetime.fromtimestamp(int(df['date'][i][j])/1000)\n",
    "                diff = (second-first).seconds\n",
    "                latlist.append(diff)\n",
    "                count += 1\n",
    "    latency.append(latlist)\n",
    "                \n",
    "df[\"latency\"] = latency\n",
    "print(\"Number of responses: \" + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participants: 80\n"
     ]
    }
   ],
   "source": [
    "#group by participant\n",
    "\n",
    "idlist = []\n",
    "parlist = []\n",
    "latencylist = []\n",
    "score = []\n",
    "\n",
    "for p in set(df['id']):\n",
    "    tempdf = df[df['id'] == p]\n",
    "    tempdf = tempdf.reset_index()\n",
    "    idlist.append(p)\n",
    "    score.append(tempdf['score'][0])\n",
    "    parlist.append(tempdf.shape[0])\n",
    "    lat = []\n",
    "    for i in range(0, tempdf.shape[0]):\n",
    "        for j in tempdf.latency[i]:\n",
    "            lat.append(j)\n",
    "    latencylist.append(lat)\n",
    "\n",
    "newdf = pd.DataFrame()\n",
    "newdf['id'] = idlist\n",
    "newdf['contacts'] = parlist\n",
    "newdf['latency'] = latencylist\n",
    "newdf['score'] = score\n",
    "print(\"Participants: \" + str(newdf.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#extract latency features\n",
    "\n",
    "responses = []\n",
    "minlist = []\n",
    "quant10 = []\n",
    "quant25 = []\n",
    "quant50 = []\n",
    "quant75 = []\n",
    "quant90 = []\n",
    "maxlist = []\n",
    "median = []\n",
    "\n",
    "for i in range(0, newdf.shape[0]):\n",
    "    responses.append(len(newdf.latency[i]))\n",
    "    minlist.append(min(newdf.latency[i]))\n",
    "    quant10.append(np.quantile(newdf.latency[i], 0.1))\n",
    "    quant25.append(np.quantile(newdf.latency[i], 0.25))\n",
    "    quant50.append(np.quantile(newdf.latency[i], 0.5))\n",
    "    quant75.append(np.quantile(newdf.latency[i], 0.75))\n",
    "    quant90.append(np.quantile(newdf.latency[i], 0.9))\n",
    "    maxlist.append(max(newdf.latency[i]))\n",
    "    median.append(np.mean(newdf.latency[i]))\n",
    "    \n",
    "newdf['responses'] = responses\n",
    "newdf['min'] = minlist\n",
    "newdf['quant10'] = quant10\n",
    "newdf['quant25'] = quant25\n",
    "newdf['quant50'] = quant50\n",
    "newdf['quant75'] = quant75\n",
    "newdf['quant90'] = quant90\n",
    "newdf['max'] = maxlist\n",
    "newdf['mean'] = median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newdf.to_csv(\"Latency\" + modality + str(ndays) + \".csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
